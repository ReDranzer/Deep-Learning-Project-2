# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lvpiAMawy16XSdEANq2TAlkzrozKXSMw
"""

from google.colab import drive
drive.mount('/content/gdrive')
import torch
import torchvision

import keras
import keras.backend as K
from keras.models import Sequential
from keras.layers import Dense

torchvision.datasets.FashionMNIST(' ./content/gdrive', train=True, transform=None, target_transform=None, download=True)
trainX,trainY = torch.load(" ./content/gdrive/FashionMNIST/processed/training.pt")
testX,testY = torch.load(" ./content/gdrive/FashionMNIST/processed/test.pt")
from torch.autograd import Variable
print(len(trainY))
!python --version

NUM_DIGITS = 784
NUM_HIDDEN1 = 150
NUM_HIDDEN2 = 150
NUM_HIDDEN3 = 150
BATCH_SIZE = 16
trainX= Variable(torch.Tensor.float((trainX)))
print((trainX).size())
#trainY=Variable(torch.LongTensor(trainY))
#print(trainY)
model = torch.nn.Sequential(
    torch.nn.Linear(NUM_DIGITS, NUM_HIDDEN1),
    torch.nn.ReLU(),
    torch.nn.Linear(NUM_HIDDEN1,NUM_HIDDEN2),
    torch.nn.ReLU(),
    torch.nn.Linear(NUM_HIDDEN2,NUM_HIDDEN3),    
    torch.nn.ReLU(),
    torch.nn.Linear(NUM_HIDDEN3, 10)
)
loss_fn = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)

for epoch in range(50):
    for start in range(0, len(trainX), BATCH_SIZE):
        end = start + BATCH_SIZE
        batchX = trainX[start:end]
        batchX=batchX.view(-1,784)
        batchY = trainY[start:end]
        #print(batchX)
        y_pred = model(batchX)
        #pssssrint(y_pred)
        loss = loss_fn(y_pred, batchY)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()


    loss = loss_fn(model(trainX.view(-1,784)), trainY).data
    print ('Epoch:', epoch, 'Loss:', loss)

path = '/content/gdrive/My Drive/DL2_model/model.pth'
torch.save(model,path)

